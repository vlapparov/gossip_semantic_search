# Gossip Search Engine

This project is a semantic search engine built to allow users to search articles
from **vsd.fr** and **public.fr** websites. It uses a model for generating sentence embeddings to perform
semantic search, leveraging a **Postgres database with vector extensions** for
similarity search and a **Streamlit** frontend for an interactive user interface.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing
purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

```
python 3.9
poetry 1.5.1
```

### Installation

Third party Package installation

```bash
cd code
python -m pip install --upgrade pip
pip install poetry==1.5.1
poetry install
```

## Unittests

To run unittests, install _pytest_ in virtualenv, then run:

```bash
poetry run pytest tests
```

## Run

### Local

To launch the application locally, follow these steps:

1. _cd_ to the project root directory:

   ```bash
   cd code
   ```

2. Export the **PYTHONPATH**:

   ```bash
   export PYTHONPATH=$PYTHONPATH:$(pwd)
   ```

3. Then run:

   ```bash
   poetry run streamlit run app/app.py 
   ```

## About Data Search

The articles used in this project were parsed from historical archives of **vsd.fr** and **public.fr** via the
**Wayback Machine API**. This method allowed for retrieving content from snapshots when the original websites
were accessible. The parsed data was then processed and stored for semantic search.

## Embedding Model

The project utilizes an open-source embedding model from the **Hugging Face** `sentence-transformers` library: *
*paraphrase-MiniLM-L6-v2**.

This model is a **distilled version of BERT** and is specifically designed for **paraphrase identification and semantic
similarity tasks**. It transforms text into dense vector representations, which can then be compared using vector
similarity techniques (such as cosine similarity) to find semantically similar sentences or documents.

### Why This Model?

The **paraphrase-MiniLM-L6-v2** model was chosen for this project due to the following reasons:

- **Efficiency**: It is a smaller, lightweight version of larger transformer models, which allows it to be fast and
  memory efficient. This makes it ideal for use in production environments with limited computational resources.
- **Performance**: Despite its smaller size, the model delivers strong performance for tasks such as semantic search,
  clustering, and paraphrase mining.
- **Multilingual Support**: This model supports multiple languages, including **French**, which is crucial for our
  dataset consisting of French articles from `vsd.fr` and `public.fr`.

The embeddings generated by this model are stored in a **PostgreSQL** database with the **pgvector** extension. This
setup enables fast and efficient similarity searches over a large number of articles.

## Using PostgreSQL with Vector Extension

This project uses **PostgreSQL** with the **pgvector extension** to store and perform efficient vector similarity
searches on article embeddings. This enables fast and scalable retrieval of articles based on semantic
similarity to a user query. The vector extension in **PostgreSQL** allows for indexing and querying high-dimensional
vectors, which is crucial for large-scale semantic search tasks.

Run the following to install the pgvector extension and set up the database:

```bash
docker pull pgvector/pgvector:pg17
install postgres@17 with homebrew
```

Run the database using docker:

```bash
docker run -e POSTGRES_PASSWORD=YOUR_POSTGRES_PASSWORD -p 5432:5432 pgvector/pgvector:pg17
```

## Frontend with Streamlit

The frontend is built using **Streamlit**, a lightweight and easy-to-use framework for creating data-driven web
applications. Users can input a search query, specify the number of results to display, and interactively view
the most relevant articles. The search results are fetched in real-time from the backend, which communicates
with the **PostgreSQL** database to retrieve similar articles based on semantic embeddings.

![image](assets/app_preview.png)

## Possible Steps for Improvement

While the current system performs well for basic semantic search functionality, there are several areas where the project can be enhanced further:

### 1. **Improving the Embedding Model**

- **Explore Larger Models**: While `paraphrase-MiniLM-L6-v2` is lightweight and efficient, larger models like `sentence-transformers/paraphrase-mpnet-base-v2` or `sentence-transformers/all-mpnet-base-v2` could provide better accuracy and deeper semantic understanding at the cost of higher computational resources.
- **Fine-Tuning**: Fine-tune the embedding model specifically on a dataset of French articles to capture more domain-specific nuances and improve the quality of the embeddings for this specific use case.
- **Multilingual Models**: Investigate multilingual models like `mBERT` or `LaBSE`, which could further improve performance for French texts while retaining efficiency.

### 2. **Enhanced Data Handling and Storage**

- **Dynamic Data Updates**: Implement a pipeline to dynamically update the dataset by periodically fetching new articles from `vsd.fr` and `public.fr` (or using the Wayback Machine for historical data). This will ensure the search engine always has up-to-date content.
- **Distributed Database Setup**: If the data size grows significantly, consider implementing a distributed setup using cloud databases or partitioned storage solutions to improve scalability and performance.
  
### 3. **Search Performance Optimization**

- **Advanced Indexing**: Utilize **HNSW (Hierarchical Navigable Small World)** indexing or **FAISS (Facebook AI Similarity Search)** to accelerate the search of high-dimensional vectors in the database. This can make the system more responsive, especially with large datasets.
- **Hybrid Search**: Combine semantic search with traditional keyword-based search (e.g., ElasticSearch) to leverage the strengths of both methods. This approach would balance between exact matches for keywords and contextual similarity for broader searches.
